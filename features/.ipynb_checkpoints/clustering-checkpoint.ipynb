{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Time series features extaction to carry out a clustering using Khiva\n",
    "\n",
    "Clustering time series is a very important method for time series analysis since it allows grouping time series according to its characteristics. As an example, Given the clustering result, we can determine for instance which time series forecasting algorithms fit best to each group. This task can get over complicated, but some features extraction method could be used to simplify this task.\n",
    "\n",
    "Thanks to the features extraction ability of [Khiva](http://khiva-python.readthedocs.io/en/latest/), we can generate a [features](http://khiva-python.readthedocs.io/en/latest/khiva.html#module-khiva.features) matrix to be used as input of any of the already available clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from khiva.features import *\n",
    "from khiva.array import *\n",
    "from khiva.library import * \n",
    "from khiva.dimensionality import *\n",
    "\n",
    "import pandas as df\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, IntSlider\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we want to clusterise time series?\n",
    "\n",
    "Clustering time series can have a wide variety of uses and applications depending on the context being used.\n",
    "\n",
    "As an example, time series clustering can be used to classify them and know which time series have the same characteristics and this way, we can know which of them are suitable to concrete forecasting methods (moving average, exponential smoothing, box-jenkins, x-11, etc.).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backend\n",
    "Prints the backend being used. The CPU, CUDA and OPENCL backends are available in Khiva.  \n",
    "  \n",
    "> This interactive application is being executed in **hub.mybinder** which doesn't provide a GPU and its CPU is quite limited so it is going to take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KHIVABackend.KHIVA_BACKEND_OPENCL\n"
     ]
    }
   ],
   "source": [
    "print(get_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction\n",
    "\n",
    "In the next notebook cell, a total number of 51 features are going to be extracted from 30 time series. The mentioned time series are the result of reducing the original dimensionality to 350 points. This has been done to speed up the computation. \n",
    "\n",
    "The time series with the dimensionality reduction applied are stored in a file called `time_series_redimensioned.npy` and the code to carry out it is commented in next cell. \n",
    "\n",
    "Rembember that as this interactive application is being executed in hub.mybinder which doesn't provide a GPU and its CPU is quite limited so it is going to take some time.  This application executed in a macOS High Sierra with a 2,9 GHz Intel Core i7 processor takes around 11 seconds in extracting the features using the same backend... applied to 100 time series. It takes around 4 seconds applied to 30 time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 3.6284170150756836 seconds\n"
     ]
    }
   ],
   "source": [
    "#path = '../../energy/data/data-enerNoc/all-data/csv'\n",
    "#file_names = []\n",
    "#array_list = []\n",
    "#for filename in os.listdir(path):\n",
    "#    if \".csv\" in filename:\n",
    "#        file_names.append(filename)\n",
    "#        print(filename)\n",
    "#        data = pd.read_csv(path + \"/\" + filename)\n",
    "#        a = visvalingam(Array([range(len(data[\"value\"])), data[\"value\"].as_matrix()]), int(350))\n",
    "#        arr_tmp = a.get_col(1)\n",
    "#        array_list.append(arr_tmp.to_numpy())\n",
    "#np.save(\"time_series_redimensioned\", np.array(array_list))\n",
    "\n",
    "# Using more than 70 time series could make the application extremely slower and kill the hub.mybinder Jupyter kernel. \n",
    "\n",
    "arr_tmp = Array(np.load('./time_series_redimensioned.npy')[0:30]) # 100 time series is the maximum number of time series that could be selected.\n",
    "start = time.time()\n",
    "features = np.stack([abs_energy(arr_tmp).to_numpy(),\n",
    "                     absolute_sum_of_changes(arr_tmp).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 0).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 1).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 2).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 3).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 4).to_numpy(),\n",
    "                     aggregated_autocorrelation(arr_tmp, 5).to_numpy(),\n",
    "                     approximate_entropy(arr_tmp, 4, 0.5).to_numpy(),\n",
    "                     binned_entropy(arr_tmp, 5).to_numpy(),\n",
    "                     c3(arr_tmp, True).to_numpy(),\n",
    "                     count_above_mean(arr_tmp).to_numpy(),\n",
    "                     count_below_mean(arr_tmp).to_numpy(),\n",
    "                     cwt_coefficients(arr_tmp, Array([1, 2, 3], dtype.s32), 2, 2).to_numpy(),\n",
    "                     energy_ratio_by_chunks(arr_tmp, 2, 0).to_numpy(),\n",
    "                     first_location_of_maximum(arr_tmp).to_numpy(),\n",
    "                     first_location_of_minimum(arr_tmp).to_numpy(),\n",
    "                     has_duplicates(arr_tmp).to_numpy(),\n",
    "                     has_duplicate_max(arr_tmp).to_numpy(),\n",
    "                     has_duplicate_min(arr_tmp).to_numpy(),\n",
    "                     index_mass_quantile(arr_tmp, 0.5).to_numpy(),\n",
    "                     kurtosis(arr_tmp).to_numpy(),\n",
    "                     large_standard_deviation(arr_tmp, 0.4).to_numpy(),\n",
    "                     last_location_of_maximum(arr_tmp).to_numpy(),\n",
    "                     last_location_of_minimum(arr_tmp).to_numpy(),\n",
    "                     length(arr_tmp).to_numpy(),\n",
    "                     longest_strike_above_mean(arr_tmp).to_numpy(),\n",
    "                     longest_strike_below_mean(arr_tmp).to_numpy(),\n",
    "                     max_langevin_fixed_point(arr_tmp, 7, 2).to_numpy(),\n",
    "                     maximum(arr_tmp).to_numpy(),\n",
    "                     mean(arr_tmp).to_numpy(),\n",
    "                     mean_absolute_change(arr_tmp).to_numpy(),\n",
    "                     mean_change(arr_tmp).to_numpy(),\n",
    "                     mean_second_derivative_central(arr_tmp).to_numpy(),\n",
    "                     median(arr_tmp).to_numpy(),\n",
    "                     minimum(arr_tmp).to_numpy(),\n",
    "                     number_crossing_m(arr_tmp, 0).to_numpy(),\n",
    "                     number_cwt_peaks(arr_tmp, 2).to_numpy(),\n",
    "                     number_peaks(arr_tmp, 2).to_numpy(),\n",
    "                     percentage_of_reoccurring_datapoints_to_all_datapoints(arr_tmp, False).to_numpy(),\n",
    "                     percentage_of_reoccurring_values_to_all_values(arr_tmp, False).to_numpy(),\n",
    "                     quantile(arr_tmp, Array([0.6], dtype.f32)).to_numpy(),\n",
    "                     ratio_beyond_r_sigma(arr_tmp, 0.5).to_numpy(),\n",
    "                     ratio_value_number_to_time_series_length(arr_tmp).to_numpy(),\n",
    "                     skewness(arr_tmp).to_numpy(),\n",
    "                     standard_deviation(arr_tmp).to_numpy(),\n",
    "                     sum_of_reoccurring_values(arr_tmp).to_numpy(),\n",
    "                     sum_values(arr_tmp).to_numpy(),\n",
    "                     symmetry_looking(arr_tmp, 0.1).to_numpy(),\n",
    "                     time_reversal_asymmetry_statistic(arr_tmp, 2).to_numpy(),\n",
    "                     variance(arr_tmp).to_numpy(),\n",
    "                    ])\n",
    "\n",
    "\n",
    "print(\"Time taken: \" + str(time.time() - start) + \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we see the 51 features extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_energy(arr)</th>\n",
       "      <th>absolute_sum_of_changes(arr)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 0)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 1)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 2)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 3)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 4)</th>\n",
       "      <th>aggregated_autocorrelation(arr, 5)</th>\n",
       "      <th>approximate_entropy(arr, 4, 0.5)</th>\n",
       "      <th>binned_entropy(arr, 5)</th>\n",
       "      <th>...</th>\n",
       "      <th>quantile(arr, Array([0.6], dtype.f32))</th>\n",
       "      <th>ratio_beyond_r_sigma(arr, 0.5)</th>\n",
       "      <th>ratio_value_number_to_time_series_length(arr)</th>\n",
       "      <th>skewness(arr)</th>\n",
       "      <th>standard_deviation(arr)</th>\n",
       "      <th>sum_of_reoccurring_values(arr)</th>\n",
       "      <th>sum_values(arr)</th>\n",
       "      <th>symmetry_looking(arr, 0.1)</th>\n",
       "      <th>time_reversal_asymmetry_statistic(arr, 2)</th>\n",
       "      <th>variance(arr)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.775877e+05</td>\n",
       "      <td>3905.836182</td>\n",
       "      <td>0.016414</td>\n",
       "      <td>-0.004443</td>\n",
       "      <td>-0.327626</td>\n",
       "      <td>4.251828</td>\n",
       "      <td>0.259288</td>\n",
       "      <td>0.067230</td>\n",
       "      <td>0.666060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.955982</td>\n",
       "      <td>0.768571</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.103191</td>\n",
       "      <td>11.379580</td>\n",
       "      <td>1164.689819</td>\n",
       "      <td>10783.903320</td>\n",
       "      <td>1.0</td>\n",
       "      <td>84.218338</td>\n",
       "      <td>129.494827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.272386e+05</td>\n",
       "      <td>3745.681396</td>\n",
       "      <td>-20.097281</td>\n",
       "      <td>-5.639720</td>\n",
       "      <td>-373.588226</td>\n",
       "      <td>105.551559</td>\n",
       "      <td>66.396545</td>\n",
       "      <td>4408.501465</td>\n",
       "      <td>0.659020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>19.076759</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.277143</td>\n",
       "      <td>0.810675</td>\n",
       "      <td>12.488727</td>\n",
       "      <td>2076.668457</td>\n",
       "      <td>9768.693359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-266.422089</td>\n",
       "      <td>155.968307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.706246e+07</td>\n",
       "      <td>13226.402344</td>\n",
       "      <td>4.310038</td>\n",
       "      <td>-0.221310</td>\n",
       "      <td>-31.100117</td>\n",
       "      <td>395.437073</td>\n",
       "      <td>27.228212</td>\n",
       "      <td>741.375610</td>\n",
       "      <td>0.629267</td>\n",
       "      <td>0.98865</td>\n",
       "      <td>...</td>\n",
       "      <td>309.537659</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.597598</td>\n",
       "      <td>55.801975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95343.656250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10047.849609</td>\n",
       "      <td>3113.860352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.912509e+04</td>\n",
       "      <td>2099.667236</td>\n",
       "      <td>0.023971</td>\n",
       "      <td>0.061195</td>\n",
       "      <td>-2.393682</td>\n",
       "      <td>6.575305</td>\n",
       "      <td>0.890935</td>\n",
       "      <td>0.793765</td>\n",
       "      <td>0.605621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>23.098700</td>\n",
       "      <td>0.788571</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>1.032005</td>\n",
       "      <td>8.361887</td>\n",
       "      <td>864.507080</td>\n",
       "      <td>3953.282715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.533302</td>\n",
       "      <td>69.921158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.968187e+04</td>\n",
       "      <td>1388.048584</td>\n",
       "      <td>-0.345298</td>\n",
       "      <td>-0.000552</td>\n",
       "      <td>-16.858433</td>\n",
       "      <td>6.790436</td>\n",
       "      <td>2.845085</td>\n",
       "      <td>8.094508</td>\n",
       "      <td>0.658300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.065200</td>\n",
       "      <td>0.725714</td>\n",
       "      <td>0.254286</td>\n",
       "      <td>1.237874</td>\n",
       "      <td>4.379574</td>\n",
       "      <td>420.491272</td>\n",
       "      <td>2130.498047</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.486554</td>\n",
       "      <td>19.180672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abs_energy(arr)  absolute_sum_of_changes(arr)  \\\n",
       "0     3.775877e+05                   3905.836182   \n",
       "1     3.272386e+05                   3745.681396   \n",
       "2     2.706246e+07                  13226.402344   \n",
       "3     6.912509e+04                   2099.667236   \n",
       "4     1.968187e+04                   1388.048584   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 0)  aggregated_autocorrelation(arr, 1)  \\\n",
       "0                            0.016414                           -0.004443   \n",
       "1                          -20.097281                           -5.639720   \n",
       "2                            4.310038                           -0.221310   \n",
       "3                            0.023971                            0.061195   \n",
       "4                           -0.345298                           -0.000552   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 2)  aggregated_autocorrelation(arr, 3)  \\\n",
       "0                           -0.327626                            4.251828   \n",
       "1                         -373.588226                          105.551559   \n",
       "2                          -31.100117                          395.437073   \n",
       "3                           -2.393682                            6.575305   \n",
       "4                          -16.858433                            6.790436   \n",
       "\n",
       "   aggregated_autocorrelation(arr, 4)  aggregated_autocorrelation(arr, 5)  \\\n",
       "0                            0.259288                            0.067230   \n",
       "1                           66.396545                         4408.501465   \n",
       "2                           27.228212                          741.375610   \n",
       "3                            0.890935                            0.793765   \n",
       "4                            2.845085                            8.094508   \n",
       "\n",
       "   approximate_entropy(arr, 4, 0.5)  binned_entropy(arr, 5)      ...        \\\n",
       "0                          0.666060                     NaN      ...         \n",
       "1                          0.659020                     NaN      ...         \n",
       "2                          0.629267                 0.98865      ...         \n",
       "3                          0.605621                     NaN      ...         \n",
       "4                          0.658300                     NaN      ...         \n",
       "\n",
       "   quantile(arr, Array([0.6], dtype.f32))  ratio_beyond_r_sigma(arr, 0.5)  \\\n",
       "0                               22.955982                        0.768571   \n",
       "1                               19.076759                        0.820000   \n",
       "2                              309.537659                        0.520000   \n",
       "3                               23.098700                        0.788571   \n",
       "4                               10.065200                        0.725714   \n",
       "\n",
       "   ratio_value_number_to_time_series_length(arr)  skewness(arr)  \\\n",
       "0                                       0.880000       0.103191   \n",
       "1                                       0.277143       0.810675   \n",
       "2                                       1.000000      -0.597598   \n",
       "3                                       0.257143       1.032005   \n",
       "4                                       0.254286       1.237874   \n",
       "\n",
       "   standard_deviation(arr)  sum_of_reoccurring_values(arr)  sum_values(arr)  \\\n",
       "0                11.379580                     1164.689819     10783.903320   \n",
       "1                12.488727                     2076.668457      9768.693359   \n",
       "2                55.801975                        0.000000     95343.656250   \n",
       "3                 8.361887                      864.507080      3953.282715   \n",
       "4                 4.379574                      420.491272      2130.498047   \n",
       "\n",
       "   symmetry_looking(arr, 0.1)  time_reversal_asymmetry_statistic(arr, 2)  \\\n",
       "0                         1.0                                  84.218338   \n",
       "1                         0.0                                -266.422089   \n",
       "2                         1.0                               10047.849609   \n",
       "3                         0.0                                  33.533302   \n",
       "4                         1.0                                  -4.486554   \n",
       "\n",
       "   variance(arr)  \n",
       "0     129.494827  \n",
       "1     155.968307  \n",
       "2    3113.860352  \n",
       "3      69.921158  \n",
       "4      19.180672  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with open('filenames_file', 'wb') as fp:\n",
    "#     pickle.dump(file_names, fp)\n",
    "    \n",
    "with open ('filenames_file', 'rb') as fp:\n",
    "    file_names = pickle.load(fp)\n",
    "\n",
    "features_transposed = features.transpose()\n",
    "pandasDF = pd.DataFrame(data=features_transposed, columns=['abs_energy(arr)',\n",
    "                                                     'absolute_sum_of_changes(arr)',\n",
    "                                                     'aggregated_autocorrelation(arr, 0)',\n",
    "                                                     'aggregated_autocorrelation(arr, 1)',\n",
    "                                                     'aggregated_autocorrelation(arr, 2)',\n",
    "                                                     'aggregated_autocorrelation(arr, 3)',\n",
    "                                                     'aggregated_autocorrelation(arr, 4)',\n",
    "                                                     'aggregated_autocorrelation(arr, 5)',\n",
    "                                                     'approximate_entropy(arr, 4, 0.5)',\n",
    "                                                     'binned_entropy(arr, 5)',\n",
    "                                                     'c3(arr, True)',\n",
    "                                                     'count_above_mean(arr)',\n",
    "                                                     'count_below_mean(arr)',\n",
    "                                                     'cwt_coefficients(arr, Array([1, 2, 3], dtype.s32), 2, 2)',\n",
    "                                                     'energy_ratio_by_chunks(arr, 2, 0)',\n",
    "                                                     'first_location_of_maximum(arr)',\n",
    "                                                     'first_location_of_minimum(arr)',\n",
    "                                                     'has_duplicates(arr)',\n",
    "                                                     'has_duplicate_max(arr)',\n",
    "                                                     'has_duplicate_min(arr)',\n",
    "                                                     'index_mass_quantile(arr, 0.5)',\n",
    "                                                     'kurtosis(arr)',\n",
    "                                                     'large_standard_deviation(arr, 0.4)',\n",
    "                                                     'last_location_of_maximum(arr)',\n",
    "                                                     'last_location_of_minimum(arr)',\n",
    "                                                     'length(arr)()',\n",
    "                                                     'longest_strike_above_mean(arr)',\n",
    "                                                     'longest_strike_below_mean(arr)',\n",
    "                                                     'max_langevin_fixed_point(arr, 7, 2)',\n",
    "                                                     'maximum(arr)',\n",
    "                                                     'mean(arr)',\n",
    "                                                     'mean_absolute_change(arr)',\n",
    "                                                     'mean_change(arr)',\n",
    "                                                     'mean_second_derivative_central(arr)',\n",
    "                                                     'median(arr)',\n",
    "                                                     'minimum(arr)',\n",
    "                                                     'number_crossing_m(arr, 0)',\n",
    "                                                     'number_cwt_peaks(arr,2 )',\n",
    "                                                     'number_peaks(arr, 2)',\n",
    "                                                     'percentage_of_reoccurring_datapoints_to_all_datapoints(arr, False)',\n",
    "                                                     'percentage_of_reoccurring_values_to_all_values(arr, False)',\n",
    "                                                     'quantile(arr, Array([0.6], dtype.f32))',\n",
    "                                                     'ratio_beyond_r_sigma(arr, 0.5)',\n",
    "                                                     'ratio_value_number_to_time_series_length(arr)',\n",
    "                                                     'skewness(arr)',\n",
    "                                                     'standard_deviation(arr)',\n",
    "                                                     'sum_of_reoccurring_values(arr)',\n",
    "                                                     'sum_values(arr)',\n",
    "                                                     'symmetry_looking(arr, 0.1)',\n",
    "                                                     'time_reversal_asymmetry_statistic(arr, 2)',\n",
    "                                                     'variance(arr)'])\n",
    "pandasDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering \n",
    "\n",
    "The 51 features calculated in the previous cell are used to clusterize the time series. The desired number of clusters can be chosen, as the number of time series to be shown in the plot. The time series are tagged with the file names they belong to.\n",
    "\n",
    "In order to plot the results, we apply PCA to use 3 principal components.\n",
    "\n",
    "> Note: In order to carry out the clustering, a k-means clustering method from Scikit-learn has been used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=1, continuous_update=False, description='Clusters', max=30, min=1), IntS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Converts the Pandas dataFrame to a numpy array.\n",
    "X = pandasDF.as_matrix()\n",
    "# Converts NaN to 0.\n",
    "X = np.nan_to_num(X)\n",
    "# Preprocess the Features matrix by scaling it.\n",
    "X = scale(X)\n",
    "# Applies a Principal Component Analysis with a desired number of components equal to three.\n",
    "pca = PCA(n_components=3)\n",
    "# Create a Pandas dataFrame composed by the principal components.\n",
    "principal_components = pca.fit_transform(X)\n",
    "principal_df = pd.DataFrame(data = principal_components\n",
    "             , columns = ['PC-1', 'PC-2', 'PC-3'])\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "# Plots the results of applying Kmeans with the desired number of \n",
    "# clusters and the desired number of time series to be shown in the plot\n",
    "def run_prediction(Clusters, TS):\n",
    "    kmeans = KMeans(n_clusters=int(Clusters), random_state=0).fit(X)\n",
    "    fig  = plt.figure()\n",
    "    ax= Axes3D(fig)\n",
    "    ax.scatter(xs=principal_df['PC-1'].as_matrix()[0:TS], \n",
    "               ys = principal_df['PC-2'].as_matrix()[0:TS], \n",
    "               zs=principal_df['PC-3'].as_matrix()[0:TS], \n",
    "               c=kmeans.labels_[0:TS], s=250) \n",
    "\n",
    "    for i, txt in enumerate(file_names[0:TS], 0):\n",
    "        ax.text(principal_df['PC-1'][i],\n",
    "                principal_df['PC-2'][i],principal_df['PC-3'][i],\n",
    "                '%s' % (str(txt)), size=20, zorder=1,  color='k')\n",
    "\n",
    "    ax.set_xlabel('PC-1')\n",
    "    ax.set_ylabel('PC-2')\n",
    "    ax.set_zlabel('PC-3')\n",
    "    plt.tight_layout\n",
    "    plt.show()\n",
    "    \n",
    "interact(run_prediction,TS=IntSlider(min=1, max=len(X), step=1, value = 1,  continuous_update=False), Clusters=IntSlider(min=1, max=len(X), step=1, continuous_update=False));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
